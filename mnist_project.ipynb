{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "valid_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_dataset))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std = 1.0):\n",
    "    (torch.randn(size) * std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dl = DataLoader(dataset=valid_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img, label = valid_dataset[1000]\n",
    "img_pil = transforms.ToPILImage()(img)\n",
    "img_pil.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28, 1))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: ./data/MNIST\n",
      "Subdirectories: ['raw']\n",
      "Files: []\n",
      "Root: ./data/MNIST/raw\n",
      "Subdirectories: []\n",
      "Files: ['t10k-images-idx3-ubyte', 't10k-labels-idx1-ubyte', 'train-images-idx3-ubyte', 't10k-images-idx3-ubyte.gz', 'train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz', 'train-labels-idx1-ubyte', 't10k-labels-idx1-ubyte.gz']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "dataset_path = './data/MNIST'\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(\"Root:\", root)\n",
    "    print(\"Subdirectories:\", dirs)\n",
    "    print(\"Files:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0: 5923 images\n",
      "Digit 1: 6742 images\n",
      "Digit 2: 5958 images\n",
      "Digit 3: 6131 images\n",
      "Digit 4: 5842 images\n",
      "Digit 5: 5421 images\n",
      "Digit 6: 5918 images\n",
      "Digit 7: 6265 images\n",
      "Digit 8: 5851 images\n",
      "Digit 9: 5949 images\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels = [label for _, label in train_dataset]\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "for digit, count in sorted(label_counts.items()):\n",
    "    print(f\"Digit {digit}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0 tensor shape: torch.Size([5923, 28, 28])\n",
      "Digit 1 tensor shape: torch.Size([6742, 28, 28])\n",
      "Digit 2 tensor shape: torch.Size([5958, 28, 28])\n",
      "Digit 3 tensor shape: torch.Size([6131, 28, 28])\n",
      "Digit 4 tensor shape: torch.Size([5842, 28, 28])\n",
      "Digit 5 tensor shape: torch.Size([5421, 28, 28])\n",
      "Digit 6 tensor shape: torch.Size([5918, 28, 28])\n",
      "Digit 7 tensor shape: torch.Size([6265, 28, 28])\n",
      "Digit 8 tensor shape: torch.Size([5851, 28, 28])\n",
      "Digit 9 tensor shape: torch.Size([5949, 28, 28])\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "digit_tensors = {digit: [] for digit in range(10)}\n",
    "\n",
    "for img, label in train_dataset:\n",
    "    digit_tensors[label].append(img)\n",
    "#print(digit_tensors[0])\n",
    "digit_tensors = {digit: torch.stack(images).squeeze(1).view(-1, 28, 28) for digit, images in digit_tensors.items()}\n",
    "#print(digit_tensors[0])\n",
    "for digit,tensor in digit_tensors.items():\n",
    "    print(f\"Digit {digit} tensor shape: {tensor.shape}\")\n",
    "\n",
    "print(digit_tensors.keys())\n",
    "\n",
    "#zero_tensors = [tensor(Image.open(o)) for o in zer0s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for digit in digit_tensors:\n",
    "    meanie = digit_tensors[digit].mean(0)\n",
    "    #print(\"Mean image: \" + meanie)\n",
    "    img_p = transforms.ToPILImage()(meanie)\n",
    "    img_p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0 tensor shape: torch.Size([980, 28, 28])\n",
      "Digit 1 tensor shape: torch.Size([1135, 28, 28])\n",
      "Digit 2 tensor shape: torch.Size([1032, 28, 28])\n",
      "Digit 3 tensor shape: torch.Size([1010, 28, 28])\n",
      "Digit 4 tensor shape: torch.Size([982, 28, 28])\n",
      "Digit 5 tensor shape: torch.Size([892, 28, 28])\n",
      "Digit 6 tensor shape: torch.Size([958, 28, 28])\n",
      "Digit 7 tensor shape: torch.Size([1028, 28, 28])\n",
      "Digit 8 tensor shape: torch.Size([974, 28, 28])\n",
      "Digit 9 tensor shape: torch.Size([1009, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "valid_digit_tensors = {digit: [] for digit in range(10)}\n",
    "\n",
    "for img, label in valid_dataset:\n",
    "    valid_digit_tensors[label].append(img)\n",
    "\n",
    "valid_digit_tensors = {digit: torch.stack(images).squeeze(1).view(-1, 28, 28) for digit, images in valid_digit_tensors.items()}\n",
    "\n",
    "for digit,tensor in valid_digit_tensors.items():\n",
    "    print(f\"Digit {digit} tensor shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for digit in valid_digit_tensors:\n",
    "    meanie = valid_digit_tensors[digit].mean(0)\n",
    "    #print(\"Mean image: \" + meanie)\n",
    "    img_p = transforms.ToPILImage()(meanie)\n",
    "    img_p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds > 0.5) == yb\n",
    "    return correct.float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb.view(xb.size(0), -1)), yb) for xb, yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "linear_model = nn.Linear(28*28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b = linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params, self.lr = list(params), lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "    \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(), 1.)\n",
    "\n",
    "def train_epoch(model):\n",
    "    for xb,yb in train_dl:\n",
    "        xb = xb.view(xb.size(0), -1)\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.0980.098"
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(simple_net.parameters(), lr=0.01)\n",
    "\n",
    "def train_model2(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        simple_net.train()\n",
    "        train_loss = 0\n",
    "        for xb,yb in train_dl:\n",
    "            xb = xb.view(xb.size(0), -1)\n",
    "            preds = simple_net(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        simple_net.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_dl:\n",
    "                xb = xb.view(xb.size(0), -1)\n",
    "                preds = simple_net(xb)\n",
    "                val_loss += criterion(preds, yb).item()\n",
    "                predicted = preds.argmax(dim = 1)\n",
    "                correct += (predicted == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "\n",
    "        train_loss /= len(train_dl)\n",
    "        val_loss /= len(valid_dl)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:4f}, Val Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  Train Loss: 0.0667\n",
      "  Val Loss: 0.111105, Val Accuracy: 0.9681\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0661\n",
      "  Val Loss: 0.111100, Val Accuracy: 0.9678\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0648\n",
      "  Val Loss: 0.108463, Val Accuracy: 0.9681\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0641\n",
      "  Val Loss: 0.111206, Val Accuracy: 0.9676\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0632\n",
      "  Val Loss: 0.107808, Val Accuracy: 0.9683\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0624\n",
      "  Val Loss: 0.108938, Val Accuracy: 0.9671\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0614\n",
      "  Val Loss: 0.108378, Val Accuracy: 0.9689\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0606\n",
      "  Val Loss: 0.109501, Val Accuracy: 0.9682\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0600\n",
      "  Val Loss: 0.109362, Val Accuracy: 0.9686\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0593\n",
      "  Val Loss: 0.108578, Val Accuracy: 0.9685\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0583\n",
      "  Val Loss: 0.109329, Val Accuracy: 0.9690\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0575\n",
      "  Val Loss: 0.113157, Val Accuracy: 0.9666\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0570\n",
      "  Val Loss: 0.109111, Val Accuracy: 0.9681\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0562\n",
      "  Val Loss: 0.111455, Val Accuracy: 0.9675\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0556\n",
      "  Val Loss: 0.108713, Val Accuracy: 0.9686\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0547\n",
      "  Val Loss: 0.110059, Val Accuracy: 0.9681\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0541\n",
      "  Val Loss: 0.111156, Val Accuracy: 0.9673\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0533\n",
      "  Val Loss: 0.109714, Val Accuracy: 0.9690\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0528\n",
      "  Val Loss: 0.111547, Val Accuracy: 0.9677\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0521\n",
      "  Val Loss: 0.113945, Val Accuracy: 0.9673\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0515\n",
      "  Val Loss: 0.110727, Val Accuracy: 0.9689\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0507\n",
      "  Val Loss: 0.113310, Val Accuracy: 0.9669\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0502\n",
      "  Val Loss: 0.112150, Val Accuracy: 0.9676\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0495\n",
      "  Val Loss: 0.111144, Val Accuracy: 0.9687\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0491\n",
      "  Val Loss: 0.112707, Val Accuracy: 0.9672\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0487\n",
      "  Val Loss: 0.109843, Val Accuracy: 0.9682\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0478\n",
      "  Val Loss: 0.111438, Val Accuracy: 0.9668\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0473\n",
      "  Val Loss: 0.111699, Val Accuracy: 0.9692\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0468\n",
      "  Val Loss: 0.111538, Val Accuracy: 0.9673\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0462\n",
      "  Val Loss: 0.111396, Val Accuracy: 0.9681\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0458\n",
      "  Val Loss: 0.111787, Val Accuracy: 0.9670\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0452\n",
      "  Val Loss: 0.112090, Val Accuracy: 0.9678\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0447\n",
      "  Val Loss: 0.115472, Val Accuracy: 0.9669\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0443\n",
      "  Val Loss: 0.111812, Val Accuracy: 0.9668\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0440\n",
      "  Val Loss: 0.112127, Val Accuracy: 0.9671\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0433\n",
      "  Val Loss: 0.112866, Val Accuracy: 0.9680\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0429\n",
      "  Val Loss: 0.112425, Val Accuracy: 0.9672\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0423\n",
      "  Val Loss: 0.113094, Val Accuracy: 0.9676\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0419\n",
      "  Val Loss: 0.113888, Val Accuracy: 0.9664\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0414\n",
      "  Val Loss: 0.113705, Val Accuracy: 0.9671\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0412\n",
      "  Val Loss: 0.112391, Val Accuracy: 0.9678\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0404\n",
      "  Val Loss: 0.113804, Val Accuracy: 0.9675\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0400\n",
      "  Val Loss: 0.114127, Val Accuracy: 0.9674\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0397\n",
      "  Val Loss: 0.113728, Val Accuracy: 0.9672\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0391\n",
      "  Val Loss: 0.115009, Val Accuracy: 0.9673\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0389\n",
      "  Val Loss: 0.113521, Val Accuracy: 0.9683\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0386\n",
      "  Val Loss: 0.115135, Val Accuracy: 0.9681\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0380\n",
      "  Val Loss: 0.113526, Val Accuracy: 0.9687\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0376\n",
      "  Val Loss: 0.115916, Val Accuracy: 0.9681\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0373\n",
      "  Val Loss: 0.116902, Val Accuracy: 0.9675\n"
     ]
    }
   ],
   "source": [
    "train_model2(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 1\n"
     ]
    }
   ],
   "source": [
    "simple_net.eval()\n",
    "\n",
    "image_path = \"three_img.png\"\n",
    "image = Image.open(image_path).convert('L')\n",
    "\n",
    "testing_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "image_tensor = testing_transform(image).unsqueeze(0)\n",
    "# test_img_pil = transforms.ToPILImage()(image_tensor)\n",
    "# test_img_pil.show()\n",
    "image_tensor = image_tensor.view(image_tensor.size(0), -1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = simple_net(image_tensor)\n",
    "    predicted_label = predictions.argmax(dim=1).item()\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashiongan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
